{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wKr_F0XBTvV"
      },
      "source": [
        "# Text generation with a miniature GPT\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/29<br>\n",
        "**Last modified:** 2020/05/29<br>\n",
        "**Description:** Implement a miniature version of GPT and train it to generate text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biXp5ZAHBTvW"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to implement an autoregressive language model\n",
        "using a miniature version of the GPT model.\n",
        "The model consists of a single Transformer block with causal masking\n",
        "in its attention layer.\n",
        "We use the text from the IMDB sentiment classification dataset for training\n",
        "and generate new movie reviews for a given prompt.\n",
        "When using this script with your own dataset, make sure it has at least\n",
        "1 million words.\n",
        "\n",
        "This example should be run with `tf-nightly>=2.3.0-dev20200531` or\n",
        "with TensorFlow 2.3 or higher.\n",
        "\n",
        "**References:**\n",
        "\n",
        "- [GPT](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)\n",
        "- [GPT-2](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)\n",
        "- [GPT-3](https://arxiv.org/abs/2005.14165)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pdgLri8BTvW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "okpr6TTVBTvW"
      },
      "outputs": [],
      "source": [
        "# We set the backend to TensorFlow. The code works with\n",
        "# both `tensorflow` and `torch`. It does not work with JAX\n",
        "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
        "# (used in `causal_attention_mask()`: `tile` in JAX does\n",
        "# not support a dynamic `reps` argument.\n",
        "# You can make the code work in JAX by wrapping the\n",
        "# inside of the `causal_attention_mask` function in\n",
        "# a decorator to prevent jit compilation:\n",
        "# `with jax.ensure_compile_time_eval():`.\n",
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import random\n",
        "import tensorflow\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3er8NGvBTvX"
      },
      "source": [
        "## Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rnl6dKYXBTvX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = ops.arange(n_dest)[:, None]\n",
        "    j = ops.arange(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = ops.cast(m, dtype)\n",
        "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = ops.concatenate(\n",
        "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
        "    )\n",
        "    return ops.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE45qrRdBTvX"
      },
      "source": [
        "## Implement an embedding layer\n",
        "\n",
        "Create two separate embedding layers: one for tokens and one for token index\n",
        "(positions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HiW_hhawBTvX"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = ops.shape(x)[-1]\n",
        "        positions = ops.arange(0, maxlen, 1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QTSnNKyBTvY"
      },
      "source": [
        "## Implement the miniature GPT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wFq7W3rZBTvY"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 80  # Max sequence size\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block(x)\n",
        "    outputs = layers.Dense(vocab_size)(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        \"adam\",\n",
        "        loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXqj5og2BTvY"
      },
      "source": [
        "## Prepare the data for word-level language modelling\n",
        "\n",
        "Download the IMDB dataset and combine training and validation sets for a text\n",
        "generation task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xidERr0GBTvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398d222e-9ea6-4709-f947-3859ea4ce96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  42.4M      0  0:00:01  0:00:01 --:--:-- 42.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OBzLXaNDBTvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cb3392-d7a0-4584-df59-f7d265be983c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000 files\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 128\n",
        "\n",
        "# The dataset contains each review in a separate text file\n",
        "# The text files are present in four different folders\n",
        "# Create a list all files\n",
        "filenames = []\n",
        "directories = [\n",
        "    \"aclImdb/train/pos\",\n",
        "    \"aclImdb/train/neg\",\n",
        "    \"aclImdb/test/pos\",\n",
        "    \"aclImdb/test/neg\",\n",
        "]\n",
        "for dir in directories:\n",
        "    for f in os.listdir(dir):\n",
        "        filenames.append(os.path.join(dir, f))\n",
        "\n",
        "print(f\"{len(filenames)} files\")\n",
        "\n",
        "# Create a dataset from text files\n",
        "random.shuffle(filenames)\n",
        "text_ds = tf_data.TextLineDataset(filenames)\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
        "    lowercased = tf_strings.lower(input_string)\n",
        "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
        "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
        "\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
        "\n",
        "\n",
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tensorflow.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
        "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x85YGdUyBTvY"
      },
      "source": [
        "## Implement a Keras callback for generating text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2EV3YUt8BTvY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x, verbose=0)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"this movie is\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpIjDc1LBTvY"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Note: This code should preferably be run on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0BChadYhBTvY"
      },
      "outputs": [],
      "source": [
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating carbon emissions for tracking"
      ],
      "metadata": {
        "id": "EmxdClTWB7qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install carbontracker\n",
        "from carbontracker.tracker import CarbonTracker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXyStiFiB5VG",
        "outputId": "f4097fbb-7e8b-473b-9a8d-8f922d6dac8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting carbontracker\n",
            "  Downloading carbontracker-2.1.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from carbontracker) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from carbontracker) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from carbontracker) (2.2.2)\n",
            "Requirement already satisfied: geocoder in /usr/local/lib/python3.11/dist-packages (from carbontracker) (1.38.1)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from carbontracker) (12.0.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from carbontracker) (7.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from carbontracker) (8.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from geocoder->carbontracker) (8.1.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from geocoder->carbontracker) (1.0.0)\n",
            "Requirement already satisfied: ratelim in /usr/local/lib/python3.11/dist-packages (from geocoder->carbontracker) (0.1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geocoder->carbontracker) (1.17.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->carbontracker) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->carbontracker) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->carbontracker) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->carbontracker) (2025.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->carbontracker) (12.570.86)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->carbontracker) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->carbontracker) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->carbontracker) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->carbontracker) (2025.4.26)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ratelim->geocoder->carbontracker) (4.4.2)\n",
            "Downloading carbontracker-2.1.2-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: carbontracker\n",
            "Successfully installed carbontracker-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tracker for 25 epochs\n",
        "tracker = CarbonTracker(epochs=25)\n",
        "\n",
        "for epoch in range(25):\n",
        "    tracker.epoch_start()  # start tracking this epoch\n",
        "\n",
        "    model.fit(\n",
        "        text_ds,\n",
        "        verbose=2,\n",
        "        epochs=1,  # train for one epoch at a time\n",
        "        callbacks=[text_gen_callback]\n",
        "    )\n",
        "\n",
        "    tracker.epoch_end()  # end tracking this epoch\n",
        "\n",
        "tracker.stop()  # stop the tracker after all epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgMS6g4K7Ao",
        "outputId": "e87e37e1-875d-42ce-c6aa-cd3a81898b17"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CarbonTracker: The following components were found: GPU with device(s) Tesla T4.\n",
            "CarbonTracker: WARNING - ElectricityMaps API key not set. Will default to average carbon intensity.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 369.47318 gCO2/kWh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated text:\n",
            "this movie is a bad horror movie . the main reason i am so annoyed to see a few of her acting abilities in this film were only the same . i don 't know why . it was because i was a kid\n",
            "\n",
            "391/391 - 62s - 157ms/step - loss: 3.3054\n",
            "CarbonTracker: WARNING - ElectricityMaps API key not set. Will default to average carbon intensity.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 369.47318 gCO2/kWh.\n",
            "CarbonTracker: Live carbon intensity could not be fetched at detected location: The Dalles, Oregon, US. Defaulted to average carbon intensity for US in 2023 of 369.47 gCO2/kWh. at detected location: The Dalles, Oregon, US.\n",
            "CarbonTracker: \n",
            "Predicted consumption for 25 epoch(s):\n",
            "\tTime:\t0:25:39\n",
            "\tEnergy:\t0.046228476450 kWh\n",
            "\tCO2eq:\t17.080182200630 g\n",
            "\tThis is equivalent to:\n",
            "\t0.158885415820 km travelled by car\n",
            "generated text:\n",
            "this movie is just a waste of time on it and i have to say that i am surprised it has no redeeming [UNK] \" in it . i think that is that this movie is so bad that it is not bad .\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.2887\n",
            "generated text:\n",
            "this movie is a complete waste of time . i can only be found out i really like a really bad movie . it is the kind of way , because it would have been nice . i don 't like the [UNK] .\n",
            "\n",
            "391/391 - 80s - 205ms/step - loss: 3.2724\n",
            "generated text:\n",
            "this movie is one of my all -time favorite movies , it has a great soundtrack . i can 't be it . this movie is great . i am a big fan of this movie . . it has the music is a\n",
            "\n",
            "391/391 - 64s - 163ms/step - loss: 3.2567\n",
            "generated text:\n",
            "this movie is a terrible , and it 's really cheesy at a local movie theater . it is just plain boring . it 's a terrible movie that should have been cut by a long flashback which is what it 's not the\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.2421\n",
            "generated text:\n",
            "this movie is so good . . the movie starts out with some sort of [UNK] [UNK] [UNK] , \" it 's not bad but i think the way he 's playing with the [UNK] \" character . but i guess this is one\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.2284\n",
            "generated text:\n",
            "this movie is an absolute waste of money . i have not read the book . . . i like those people who think it should give me some credit to the film . it was a [UNK] . and it 's the story\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.2147\n",
            "generated text:\n",
            "this movie is a great movie , with many layers of social realism , social realism . this movie is a story about loyalty , a love story between a young man and his friends and one life in a heart beats and tears\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.2020\n",
            "generated text:\n",
            "this movie is one of the most boring and uninteresting . the movie is not that great . the acting is great and a very good movie . it has no plot . i am not sure how to make it , but this\n",
            "\n",
            "391/391 - 63s - 162ms/step - loss: 3.1901\n",
            "generated text:\n",
            "this movie is one of the most boring and boring . the only thing i saw about half an hour long time ago . i 'm a big fan of the movie that i am surprised that i can say that this movie had\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 3.1779\n",
            "generated text:\n",
            "this movie is a complete waste of time . it was so awful i thought it was a very entertaining movie . i have to admit that i am a bit more than the typical indian film . i have the story is a\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 3.1669\n",
            "generated text:\n",
            "this movie is about a guy named jeff who has been on a [UNK] in the us world . he is an excellent one but he gets to [UNK] to [UNK] his own [UNK] , he has no business messing up with . no\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.1561\n",
            "generated text:\n",
            "this movie is the greatest movie ever made . it is not for everyone , but not in this movie , because you will be able to hang out in their own right . the movie has a very well done , and shows\n",
            "\n",
            "391/391 - 63s - 162ms/step - loss: 3.1458\n",
            "generated text:\n",
            "this movie is an absolute waste of time in a time . the acting is great , the directing in my life . the story is told from this movie . i was so upset that it is a [UNK] that could have been\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.1355\n",
            "CarbonTracker: WARNING - ElectricityMaps API key not set. Will default to average carbon intensity.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 369.47318 gCO2/kWh.\n",
            "generated text:\n",
            "this movie is a masterpiece . it is very interesting to see what is the movie . it is really great . i really like most other people i have to say , but i love the storyline is just plain old . i\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.1259\n",
            "generated text:\n",
            "this movie is not a masterpiece of art . it 's not as good as it is , but i think it is a movie that is just so bad and bad , you know why this movie isn 't the first movie .\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.1166\n",
            "generated text:\n",
            "this movie is a great movie . . . the only movie has the advantage of its that i 've ever seen . the acting , the movie is so boring and the acting is the only good thing about the plot , but\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.1075\n",
            "generated text:\n",
            "this movie is absolutely beautiful . the story has been done since it is a time [UNK] , \" it is a perfect movie . i also loved it and also shows how the dinosaurs were a true love story . i wish that\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.0988\n",
            "generated text:\n",
            "this movie is a complete waste of time . i am not expecting to say this is what the worst movie i have ever seen . my favorite movie is the most awful , i can 't stand watching it . it 's like\n",
            "\n",
            "391/391 - 63s - 160ms/step - loss: 3.0901\n",
            "generated text:\n",
            "this movie is so terrible , i really didn 't care for any of the characters . it 's not really the whole thing that happened in this film . i didn 't want to read the book , and read that it is\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.0813\n",
            "generated text:\n",
            "this movie is about viola [UNK] bynes ) , because her school girl calvin [UNK] ) and a girl who wants to get into her prestigious arts school . she gets into the school she gets into a girl but she goes to visit\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.0742\n",
            "generated text:\n",
            "this movie is really good . . i think i 'm not sure why it would be a good movie . it is just a good movie . i love the story is so much more than a nice soundtrack . i think it\n",
            "\n",
            "391/391 - 63s - 161ms/step - loss: 3.0665\n",
            "generated text:\n",
            "this movie is really bad . it is so bad . the acting is bad , bad script . the acting was bad . . . . i don 't think that 's bad , it 's bad , but it 's a terrible\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.0586\n",
            "generated text:\n",
            "this movie is just plain awful . i have a hard time believing it to them all . it 's a movie . i can see how it could manage to get past the storyline ? to tell the plot is simple and that\n",
            "\n",
            "391/391 - 62s - 160ms/step - loss: 3.0511\n",
            "generated text:\n",
            "this movie is absolutely hilarious . i love a lot and it shows on a great cast of the actors . there are great performers in this movie . the script is so bad that you can 't get any worse . the characters\n",
            "\n",
            "391/391 - 62s - 159ms/step - loss: 3.0448\n",
            "CarbonTracker: WARNING - ElectricityMaps API key not set. Will default to average carbon intensity.\n",
            "CarbonTracker: WARNING - Failed to retrieve carbon intensity: Defaulting to average carbon intensity 369.47318 gCO2/kWh.\n",
            "CarbonTracker: Average carbon intensity during training was 369.47 gCO2/kWh at detected location: The Dalles, Oregon, US.\n",
            "CarbonTracker: \n",
            "Actual consumption for 25 epoch(s):\n",
            "\tTime:\t0:26:23\n",
            "\tEnergy:\t0.046948609543 kWh\n",
            "\tCO2eq:\t17.346252064294 g\n",
            "\tThis is equivalent to:\n",
            "\t0.161360484319 km travelled by car\n",
            "CarbonTracker: Finished monitoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])"
      ],
      "metadata": {
        "id": "ANKfkRCjB4tk"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}